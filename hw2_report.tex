\documentclass{article}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{array}
\usepackage{float}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\setlength{\arrayrulewidth}{0.5mm}
\graphicspath{ {./images/} }
\usepackage[margin=20mm]{geometry}
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\renewcommand{\arraystretch}{1.5}
\newcommand{\f}[2]{f_{#1}(#2)}
\newcommand{\code}[1]{\texttt{#1}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\set}{\left\{}{\right\}}
\DeclarePairedDelimiter{\parens}{\lparen}{\rparen}
\title{HW2 Report}
\date{}
\begin{document}
\maketitle
\section*{Part 1}
\subsection*{Q2:}
    \begin{figure}[H]
        \centering
        \includegraphics{images/q2.png}
        \caption{The 10 most correlated features to \code{spread}}
    \end{figure}
\subsection*{Q3:}
    \begin{figure}[H]
        \centering
        \includegraphics{images/q3.png}
        \caption{3D Scatter plot of \code{PCR\_03}, \code{PCR\_07}, and \code{PCR\_10} according to \code{spread}}
    \end{figure}
\subsection*{Q4:}
    \paragraph*{}
    The points with label -1 for \code{spread} are contained within an ellipsoid described by the following equation:
    \begin{align*}
        \frac{PCR\_03}{a^2} + \frac{PCR\_07}{b^2} + \frac{PCR\_10}{c^2} = 1
    \end{align*}
    where $a,b,c$ are positive real numbers.
\subsection*{Q6:}
    \paragraph*{}
    Z-score scaling scales the data of a feature by ensuring that they have zero mean and unit standard deviation, thereby causing the data to adhere to a normal distribution. Features scaled according to this technique have their outliers handled correctly, but no guarantee on the resulting range of the data is made, and the ranges of different features scaled according to Z-score may differ from each other. This technique is preferable in cases that have outliers and when the learning model assumes that the data adheres to a normal distribution . On-the-other-hand, the min-max technique involves scaling the data of a feature to a specific range (generally between 0 and 1). Contrary to the Z-score method, this technique guarantees a uniform range across features and maintains the original distribution of the data, but does not handle outliers well. Therefore, it would be preferable to use this technique only when the feature in question has no significant outliers and\_or the learning model to be used requires the feature data to fall within a certain range.
\section*{Part 2}
\subsection*{Q11:}
    \begin{figure}[H]
        \centering
        \includegraphics{q11_pcr_01_pcr_02_j_plot.png}
        \caption{Joint plot of PCR\_01 and PCR\_02 according to risk}
        \label{fig:pcr_01_pcr_02_j_plot}
    \end{figure}
    \paragraph*{}
    As can be seen from the scatter portion of the jointplot in figure \ref{fig:pcr_01_pcr_02_j_plot}, the plot is mostly separable into radiuses, and therefore it seems likely that \code{PCR\_01} and \code{PCR\_02} will be important in predicting the \code{risk} class.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.4]{q11_pcr_06_01_j_plot.png}
        \caption{Joint plot of PCR\_06 versus PCR\_01 with respect to risk}
        \label{fig:pcr_06_pcr_01_j_plot}
    \end{figure}
    \paragraph*{}
     Furthermore, from the scatter portion of the jointplot in figure \ref{fig:pcr_06_pcr_01_j_plot} we noticed a mostly separable form similar to the letter "H" where the "H" itself is made up of a high proportion of points with low risk surrounded by clusters of points of high risk. Therefore, we can conclude that in addition, \code{PCR\_06} will be important in predicting the \code{risk} class.
\subsection*{Q12:}
    \begin{figure}[H]
        \centering
        \includegraphics{images/q12.png}
        \caption{The 10 most correlated features to \code{risk}}
    \end{figure}
\section*{Part 3}
\subsection*{Q20:}
    \paragraph*{}
    In Q19, we used a 2 dimensional polynomial transform on the features within the primal objective of the SVM model. This approach is slower than the two dimensional kernel objective because it involves computing inner products between data points and the $w$ vector in high dimensions, whereas in the kernel case, the dual objective is used, which only involves computing the kernel function for 2 dimensional polynomials which is of much lower dimension (and finding and storing the $\alpha$s, which is not too computationally burdensome on the assumption that the support vectors are sparse).



\end{document}